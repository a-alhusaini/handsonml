# Housing

```elixir
Mix.install([
  {:nx, "~> 0.4"},
  {:explorer, "~> 0.4"},
  {:vega_lite, "~> 0.1.5"},
  {:exla, "~> 0.5.0"},
  {:kino_vega_lite, "~> 0.1.1"},
  {:httpoison, "~> 1.8"},
  {:scholar, github: "elixir-nx/scholar"}
])

Nx.global_default_backend(EXLA.Backend)
```

## Fetching The Dataset

```elixir
defmodule FetchDataSet do
  @download_root "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
  @housing_path Path.join(["ch2", "datasets", "housing"])
  @housing_url @download_root <> "datasets/housing/housing.tgz"

  def run(housing_url \\ @housing_url, housing_path \\ @housing_path) do
    if not File.dir?(housing_path) do
      File.mkdir_p!(housing_path)
    end

    resp = HTTPoison.get!(housing_url)
    tgz_path = Path.join([housing_path, "housing.tgz"])
    File.write!(tgz_path, resp.body)
    :erl_tar.extract(tgz_path, [:compressed, cwd: housing_path])

    IO.puts("Download successful")
  end
end

FetchDataSet.run()
```

## Creating a test set

The issue with creating a dataset by picking random values from the data is that there is a chance of a sampling bias occurring.

To avoid this, prefer creating the test set with stratified shuffling. Stratified shuffling is when you seperate a certain attribute into a few categories and pick 20% of each category of that attribute and use that to represent your test set.

This approach is valid in this situation. I will use the median income attribute for this stratified shuffle.

## Stratified Shuffle Implementation

```elixir
defmodule StratifiedShuffle do
  require Explorer.DataFrame

  alias Explorer.DataFrame
  alias Explorer.Series

  def run(original_df, category_column) do
    categories = DataFrame.distinct(original_df, [category_column])
    categories = Series.to_list(categories[category_column])

    %{training_df: train_set, testing_df: test_set} =
      Enum.reduce(
        categories,
        %{
          training_df: original_df,
          testing_df: original_df |> DataFrame.slice([0, 0])
        },
        fn category, %{training_df: original_df, testing_df: testing_df} ->
          testing_chunk =
            DataFrame.filter_with(original_df, fn row ->
              Series.equal(row[category_column], category)
            end)
            |> DataFrame.sample(0.2, seed: 42)

          %{
            training_df: original_df,
            testing_df: DataFrame.concat_rows(testing_df, testing_chunk)
          }
        end
      )

    remove_test_samples_from_training_set(train_set, test_set)
  end

  defp remove_test_samples_from_training_set(train_set, test_set) do
    train_mapset = DataFrame.to_rows(train_set) |> MapSet.new()
    test_mapset = DataFrame.to_rows(test_set) |> MapSet.new()

    train_set =
      MapSet.difference(train_mapset, test_mapset)
      |> MapSet.to_list()
      |> Explorer.DataFrame.new()

    {train_set, test_set}
  end
end
```

## train/test split implementation

```elixir
defmodule SplitTrainTest do
  require Explorer.DataFrame

  alias Explorer.DataFrame

  def run() do
    full_set = DataFrame.from_csv!("./ch2/datasets/housing/housing.csv")

    # add income_categorization to dataset
    full_set =
      DataFrame.put(
        full_set,
        "income_cat",
        Enum.map(
          full_set["median_income"]
          |> Explorer.Series.to_enum(),
          fn income ->
            cond do
              income < 1.5 -> 1
              income < 3.0 -> 2
              income < 4.5 -> 3
              income < 6.0 -> 4
              true -> 5
            end
          end
        )
      )

    StratifiedShuffle.run(full_set, "income_cat")
  end
end

{train_split, test_split} = SplitTrainTest.run()

# get rid of income_cat. That was only for creating stratified shuffle
clear_cat = fn v -> Explorer.DataFrame.discard(v, "income_cat") end
train_split = clear_cat.(train_split)
test_split = clear_cat.(test_split)
```

## Data Exploration Phase

After spliting off a test set we can start working on exploring the available data.

Some things to do

* Plot the data to find patterns (scatter matrixes, boxplots, charts, graphs... blah blah blah)
* Look at linear and nonlinear corelations related to the data (review pearson's r to get linear corelations)
* Derive values from the original values and see if there are any interesting things to discover about the derived data points (eg: bedrooms/household...)

## Data Transformation Pipeline

After fiddling around with the data and getting a feel for what your data looks like and identifying some possible relationships you can now start transforming that data into something a machine learning model will be able to understand.

The generally involves 3 sorts of tasks

* Handling missing values - Either delete them or fill them with a value like the mean
* Encoding binary features - Like distance_from_ocean. Use one_hot encoding for this
* handling numeric values - house_value and house_median_income vary wildly. Use regularization/standardization to fix that

```elixir
# convert ocean proximity to a number
feature_to_number =
  Explorer.Series.distinct(train_split["ocean_proximity"])
  |> Explorer.Series.to_enum()
  |> Enum.reduce([], fn n, acc -> acc ++ ["#{n}": length(acc)] end)

encoded_ocean_proximity =
  Explorer.Series.transform(train_split["ocean_proximity"], fn colname ->
    feature_to_number[:"#{colname}"]
  end)

train_split =
  train_split
  |> Explorer.DataFrame.put("ocean_proximity", encoded_ocean_proximity)

# convert nil to nan so NX understands them
train_split =
  train_split
  |> Explorer.DataFrame.names()
  |> Enum.reduce(train_split, fn colname, split ->
    if Explorer.Series.dtype(split[colname]) == :float do
      Explorer.DataFrame.put(split, colname, Explorer.Series.fill_missing(split[colname], :nan))
    else
      split
    end
  end)

# TODO: add one hot encoding for ocean_proximity 

# Explorer is no longer needed. 
# Everything from here on out will be done with Nx. Convert data to Nx

train_label_split =
  train_split["median_house_value"]
  |> Explorer.Series.to_tensor()

train_split =
  Explorer.DataFrame.discard(train_split, "median_house_value")
  |> Nx.stack(axis: 1)

# Handle missing values in training data with the median
train_split =
  train_split
  |> Scholar.Impute.SimpleImputer.fit(strategy: :median)
  |> Scholar.Impute.SimpleImputer.transform(train_split)

model = Scholar.Linear.LinearRegression.fit(train_split, train_label_split)
```

```elixir
predictions = Scholar.Linear.LinearRegression.predict(model, train_split)
Scholar.Metrics.mean_absolute_error(train_label_split, predictions)
```
